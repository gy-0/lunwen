\prefacesection{Lay Abstract}

This thesis presents the development of an intelligent text-to-image transformation system that converts textual content from images into high-quality visual representations. The system addresses the challenge of bridging the gap between written information and meaningful visual outputs through a comprehensive three-stage pipeline.

First, advanced image preprocessing techniques are applied to enhance image quality, followed by custom-trained Tesseract OCR models that accurately extract text from diverse image sources including handwritten content, printed documents, and complex layouts. Second, the extracted text is processed by a locally deployed GPT-OSS-20B language model using the Ollama framework, which corrects OCR errors and transforms raw text into semantically rich, contextually appropriate prompts optimized for image generation. Finally, these enhanced prompts are used with FLUX 1.1 Pro Ultra, a state-of-the-art diffusion model, to generate high-quality images that accurately reflect the original textual content.

The system demonstrates significant technical innovations including custom OCR model training methodologies that improve recognition accuracy by up to 15% over standard approaches, local deployment of large language models for privacy-preserving prompt optimization, and seamless integration of cloud-based image generation services within a desktop application framework. The complete pipeline achieves prompt adherence scores of 9.1/10 and maintains processing times suitable for interactive applications.

This research contributes to the fields of multimodal AI systems, document digitization, and creative content generation, providing practical applications in accessibility technologies, automated design workflows, and intelligent document processing systems.