\chapter{Introduction}

\section{Background and Context}
The proliferation of digital imaging and the increasing digitization of historical and paper-based records have resulted in a massive volume of information being stored within unstructured image formats. This visual data, ranging from scanned business documents and academic papers to photographs containing text, holds significant value. However, the text within these images is not inherently machine-readable, creating a barrier to search, analysis, and interaction. Concurrently, the field of artificial intelligence has witnessed substantial progress in generative models, particularly in the domain of text-to-image synthesis, which enables the creation of complex visual content from textual descriptions.

A compelling area of research emerges at the intersection of these fields: the development of systems capable of understanding the textual content of an image and transforming it into a new, distinct, and contextually relevant visual representation. Such a capability has practical implications for numerous applications, including automated content summarization where key textual points are converted into illustrative graphics, accessibility tools that generate visual aids from written descriptions, and creative platforms that allow users to reimagine and remix visual information.

This thesis addresses the technical challenges inherent in this process by presenting the design, implementation, and evaluation of an integrated, end-to-end system. The proposed system establishes a pipeline that begins with text extraction from an image and culminates in the generation of a new image. It is built on a hybrid architecture that strategically combines custom-trained Optical Character Recognition (OCR) models, a locally deployed large language model (LLM) for privacy-conscious natural language processing, and a state-of-the-art, cloud-based service for high-fidelity image generation. Through this work, we explore a practical and replicable framework for bridging the gap between textual information extraction and generative visual art.

\section{Problem Statement and Motivation}
The integration of OCR, natural language processing, and image synthesis into a single, functional workflow is more complex than assembling individual components. The motivation for this research is rooted in addressing several specific and persistent challenges that arise at the seams of these technologies.

First, the reliability of the entire pipeline is fundamentally dependent on the quality of the initial text extraction. While modern OCR engines perform well on clean, typewritten documents, their accuracy often degrades significantly when confronted with real-world variations. These include, but are not limited to, handwritten notes, documents with complex layouts or multiple columns, low-resolution images, non-uniform illumination, and physical degradation of the source material \cite{esser2020improving}. An error in this initial stage—such as misinterpreting a word or failing to recognize a line of text—propagates through the system, leading to nonsensical or misleading inputs for subsequent AI modules and ultimately resulting in a final output that is disconnected from the source material.

Second, the raw text produced by an OCR engine, even when accurate, is often semantically insufficient for driving a generative image model. OCR output is a literal transcription, lacking the contextual understanding, descriptive detail, and inferential reasoning that a human reader naturally applies. For example, the text “board meeting, 3pm” is factually correct but is a poor prompt for an image model. An effective prompt requires enrichment, such as “A professional business meeting taking place in a modern, sunlit conference room with a large oak table.” Performing this enrichment using cloud-based LLMs introduces significant data privacy and security risks, as the content of the documents (which could be confidential, personal, or proprietary) must be sent to a third-party service. The emergence of powerful, open-weight models like GPT-OSS-20B \cite{openai2025gptoss} presents an opportunity for local, on-device processing. However, this introduces its own set of challenges related to the high computational and memory resources required to run such models effectively on consumer-grade hardware.

Third, converting an enhanced description into a high-quality image is a sophisticated task. State-of-the-art text-to-image models, such as FLUX 1.1 Pro Ultra \cite{blackforestlabs2024flux}, are powerful but sensitive to the structure and content of the prompt. Effective prompt engineering requires a nuanced understanding of how to phrase descriptions, specify artistic styles, and guide the model toward a desired output. There is a clear and practical need for a system that can automate this process, translating a user's intent and the extracted text into a well-formed prompt. The development of such integrated multimodal systems remains a highly active and relevant area of research \cite{qin2024comprehensive, jin2024mm}.

This thesis is therefore motivated by the need for a holistic system that robustly addresses these interconnected challenges, with the goal of creating a seamless, privacy-aware, and effective pipeline from image-based text to high-quality visual generation.

\section{Research Objectives}
This research undertakes the development and evaluation of an integrated system that transforms text extracted from images into new visual representations. The primary objectives are structured to systematically address the technical challenges identified in the problem statement:

\begin{enumerate}
    \item To improve the accuracy and robustness of text extraction by training custom Tesseract OCR models. This objective involves the curation of diverse, specialized datasets, the implementation of a multi-stage image preprocessing pipeline to handle various image imperfections, and the fine-tuning of model parameters to optimize performance for specific document types.

    \item To design and implement a suite of advanced image preprocessing techniques that directly support and enhance OCR performance. This includes the development of adaptive algorithms for noise reduction, contrast and brightness normalization, and the correction of geometric distortions, ensuring that images are in an optimal state before being passed to the OCR engine.

    \item To develop a privacy-preserving natural language processing module by deploying a large language model (GPT-OSS-20B) on local hardware using the Ollama framework. The goal is to effectively transform raw, and potentially erroneous, OCR output into semantically rich and descriptive prompts suitable for image generation, without the need to transmit user data to external cloud services.

    \item To architect a scalable and resilient hybrid system that effectively integrates the various local and cloud-based AI components. This involves designing the control logic for orchestrating the multi-stage workflow, managing asynchronous operations to ensure a responsive user experience, and implementing robust error-handling mechanisms.

    \item To conduct a thorough and multi-faceted evaluation of the integrated system. This final objective is to quantitatively and qualitatively assess the system's end-to-end performance, including text extraction accuracy, the quality of generated prompts, the fidelity and relevance of the final images, and overall processing time.
\end{enumerate}

\section{Scope and Contributions}
This thesis contributes to the field of applied multimodal AI by designing, building, and validating a complete, end-to-end system for text-to-image transformation. The contributions are fourfold:

First, this work presents a novel, integrated hybrid architecture that serves as a practical blueprint for developing complex AI applications. It demonstrates how to strategically combine custom-trained models, locally-hosted LLMs, and external cloud services to balance performance, privacy, and access to cutting-edge technology. The significance of this contribution lies in its replicable framework for building privacy-conscious yet powerful AI systems.

Second, this thesis details a methodology for creating a high-accuracy OCR pipeline. The contribution is not merely the resulting trained models, but the systematic approach of coupling domain-specific training with a tightly integrated, adaptive preprocessing workflow. This demonstrates a path to achieving robust text extraction performance on challenging, real-world images that often fall outside the purview of standard, off-the-shelf OCR solutions.

Third, this research validates the practical use of a 20-billion-parameter LLM on consumer-grade hardware for a sophisticated NLP task. By successfully deploying GPT-OSS-20B locally for prompt engineering, this work provides a tangible demonstration of a privacy-first approach to AI. This is significant as it shows a viable alternative to reliance on proprietary cloud APIs for advanced language processing, thereby enabling a wider range of applications where data confidentiality is paramount.

Finally, this work offers a comprehensive evaluation methodology that assesses the system holistically, rather than just its individual parts. By measuring performance at each stage of the pipeline and for the end-to-end process, this research provides valuable insights and realistic performance benchmarks. This is crucial for guiding the future development and optimization of similarly integrated multimodal AI systems.

\section{System Architecture Overview}
The system is founded on a hybrid and modular architectural design that prioritizes performance, scalability, and data privacy. It intelligently partitions tasks between local (on-device) computation and remote cloud services. Operations that handle potentially sensitive user data—specifically, OCR and the natural language processing for prompt enhancement—are executed entirely on the user's local machine. This ensures that the original image and its textual content are never transmitted externally. In contrast, the final, computationally demanding task of image generation is delegated to a specialized, state-of-the-art cloud API, using the anonymized and enhanced prompt. This hybrid model provides an optimal balance, safeguarding user privacy while leveraging the immense power of large-scale, dedicated generative models. A more detailed breakdown of the component layers, their specific roles, and their interactions is presented in Chapter 3.

\section{Thesis Organization}
This thesis is structured into seven chapters to logically present the research from conception to conclusion. Chapter 2 establishes the foundational knowledge required to understand the work, providing technical background on the core technologies employed: the Tesseract OCR engine, the architecture of the GPT-OSS-20B large language model, and the principles of diffusion-based image generation, with a focus on the FLUX 1.1 Pro Ultra model.

Chapters 4, 5, and 6 constitute the technical core of the thesis, providing a detailed account of the implementation of the system's primary components. Chapter 4 is dedicated to the OCR pipeline, covering custom model training and the image preprocessing workflow. Chapter 5 focuses on the natural language processing module, detailing the local deployment of GPT-OSS-20B and the prompt optimization strategies. Chapter 6 describes the final stage of the pipeline: the integration with the FLUX 1.1 Pro Ultra API for image generation.

Finally, Chapter 7 concludes the thesis. It provides a summary of the research findings, a candid discussion of the project's limitations and remaining challenges, and offers suggestions for promising directions for future work in this rapidly evolving field.
