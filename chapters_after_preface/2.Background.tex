\chapter{Background}
\label{chap:background}

The transformation of textual content within images into new visual representations requires a confluence of several advanced technologies. The rise of multimodal AI, which seeks to process and reason across different data types, has provided a fertile ground for such integrated systems. A comprehensive understanding of the field necessitates a technical foundation in the core components that enable such a process. This chapter provides the requisite background, establishing a theoretical framework for the subsequent chapters. It begins with an examination of Optical Character Recognition (OCR), tracing its evolution and focusing on the architecture of the Tesseract engine. Subsequently, it delves into the development of Large Language Models (LLMs), detailing the Transformer architecture and using GPT-OSS-20B as a representative example of a modern, efficient model. The chapter concludes with a review of the state-of-the-art in text-to-image (T2I) synthesis, contrasting earlier methods with the now-dominant diffusion models, exemplified by FLUX 1.1 Pro Ultra.

\section{Optical Character Recognition with Tesseract}
\label{sec:background_ocr}
OCR is a field of computer vision and pattern recognition dedicated to the automated extraction of text from images, thereby converting unstructured visual data into a machine-readable and searchable format. The evolution of OCR technology has transitioned from early methods dependent on rigid pattern matching to sophisticated deep learning-based approaches that offer superior accuracy and robustness, particularly on varied and degraded inputs \cite{esser2020improving}. Traditional techniques often involved matrix matching or the extraction of handcrafted features (e.g., strokes, holes, contours), which were then fed into a classifier. These methods were brittle and struggled with font variations, noise, and complex layouts.

The paradigm shift towards deep learning has enabled end-to-end learning, where the model learns relevant features directly from pixel data. The Tesseract OCR engine, originally developed by Hewlett-Packard and now maintained by Google, is a prominent open-source implementation that exemplifies this modern approach. Its architecture involves a sequence of processing stages. Initially, the engine performs image preprocessing to normalize the input and enhance text clarity. This can include binarization (e.g., using Otsu's method to separate text from background), noise removal, and deskewing algorithms to correct rotational misalignment. Following this, a page layout analysis algorithm, often based on connected component analysis, identifies and isolates text regions from other elements like images or tables. These regions are then subjected to word and character segmentation.

The core of Tesseract's recognition process since version 4 is a Long Short-Term Memory (LSTM) based recurrent neural network. This is a significant departure from its previous character-by-character classification system. An LSTM is particularly well-suited for text recognition because it processes sequences of features, allowing it to learn and leverage the contextual dependencies between characters in a word and across words in a line. This contextual understanding drastically reduces errors compared to classifying each character in isolation. Finally, a post-processing stage may apply language modeling and dictionary-based corrections to refine the output and mitigate misclassification errors based on linguistic plausibility.

A significant advantage of Tesseract is its support for custom model training through transfer learning. While the engine includes pre-trained models for numerous languages, accuracy can be substantially improved for specific domains (e.g., historical documents, specialized forms) by fine-tuning the base model on a custom-labeled dataset. This process involves collecting a representative set of images, annotating them with ground-truth text, and using Tesseract's training utilities to update the neural network's weights. The resulting specialized model can achieve higher performance on challenging, domain-specific images than a general-purpose model can provide.

\section{Large Language Models for Text Processing}
\label{sec:background_llm}
The ability to understand and manipulate natural language is another cornerstone of multimodal systems. The progression of language modeling has moved from statistical methods like n-grams to neural network-based approaches. Early neural models, including Recurrent Neural Networks (RNNs) and LSTMs, processed text sequentially, which, while effective at capturing short-range dependencies, presented challenges in parallelization and in modeling very long-range context. The introduction of the Transformer architecture marked a pivotal moment, overcoming these limitations through its self-attention mechanism \cite{vaswani2017attention}.

The self-attention mechanism allows a model to weigh the importance of all other words in an input sequence when encoding a specific word, enabling it to capture complex, long-range dependencies regardless of distance. It operates on a set of Query (Q), Key (K), and Value (V) vectors derived from the input embeddings. The output for each position is a weighted sum of the Values, where the weights are determined by the compatibility (dot product) of the Query with all Keys. This parallelizable mechanism forms the foundation of most modern LLMs.

GPT-OSS-20B is an open-weight LLM from the gpt-oss series that exemplifies the modern decoder-only Transformer design \cite{openai2025gptoss}. It is a notable example of a powerful model designed with a balance of performance and suitability for local deployment.

\subsection{Architecture of GPT-OSS-20B}
GPT-OSS-20B is a decoder-only Transformer with approximately 21 billion parameters. It features a Mixture-of-Experts (MoE) routing mechanism, which is a key architectural innovation for improving model efficiency. In an MoE model, the feed-forward network layer is replaced by a set of parallel "expert" networks and a "gating" network that learns to route each input token to a small subset of these experts. This allows the model to have a very large total parameter count while keeping the computational cost for inference constant, as only a fraction of the parameters are activated for any given token. For GPT-OSS-20B, this means roughly 3.6 billion of its 21 billion parameters are active during inference. This design enhances computational throughput and improves the trade-off between latency and output quality.

\subsection{Pre-training, Alignment, and Adaptation}
The model is pre-trained on a diverse corpus of text using a causal next-token prediction objective, which endows it with broad linguistic and world knowledge. Following pre-training, an alignment phase uses supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) to improve its adherence to instructions, helpfulness, and overall safety. A key feature for local deployment is its compatibility with post-training quantization. The application of techniques like MXFP4 quantization to the MoE weights allows GPT-OSS-20B to operate within approximately 16GB of memory with minimal degradation in output quality. This facilitates its deployment on consumer-grade hardware. Released under the Apache-2.0 license, the model's open-weight nature permits further adaptation via supervised fine-tuning for domain-specific tasks.

\section{Text-to-Image Synthesis}
\label{sec:background_t2i}
The field of T2I synthesis has seen rapid advancement, with a notable shift in the underlying generative technology. Early successful approaches were dominated by Generative Adversarial Networks (GANs). GANs involve a two-player game between a generator, which creates images from random noise and a text embedding, and a discriminator, which tries to distinguish between real and generated images. Models like StackGAN and StyleGAN-T demonstrated the ability to generate realistic images, but often struggled with prompt fidelity for complex, compositional prompts and could suffer from training instability, such as mode collapse \cite{zhang2017stackgan}.

More recently, diffusion models have become the dominant architecture for high-fidelity T2I synthesis. A diffusion model operates in two stages. First, a fixed \textit{forward process} gradually adds Gaussian noise to a training image over a series of timesteps until it becomes pure, unstructured noise. Second, a \textit{reverse process} learns to undo this noising process. A neural network, typically a U-Net architecture, is trained to predict the noise that was added at each timestep and subtract it, thereby gradually denoising a random input into a coherent image. To guide this process with text, the U-Net is conditioned on a text embedding, usually produced by a frozen text encoder like CLIP \cite{radford2021learning}. This conditioning steers the denoising process toward an image that matches the semantic content of the text prompt.

FLUX 1.1 Pro Ultra is a state-of-the-art model that exemplifies the power of this paradigm \cite{blackforestlabs2024flux}.

\subsection{Core Architecture and Methodology of FLUX 1.1}
FLUX 1.1 Pro Ultra is a 12-billion-parameter model built on a hybrid architecture that combines multimodal transformers with diffusion-based generative processes. This design integrates diffusion and flow-matching techniques to improve sample efficiency and the coherence of the generated images. The architecture employs parallelized attention layers, which contribute to greater computational efficiency and faster inference times compared to traditional serial approaches. Furthermore, the inclusion of Rotary Positional Embeddings (RoPE) improves the model's spatial understanding, enabling higher fidelity in complex compositions. To enhance prompt comprehension, the model uses both textual and visual embeddings as multimodal inputs.

The training regimen for FLUX 1.1  Pro Ultra involved several advanced techniques. The training dataset was enriched through extensive data augmentation, including the use of synthetic captioning methods. A rectified flow timestep sampling approach was employed to improve training efficiency, and the model was expanded to its large parameter count in accordance with empirically derived scaling laws.

\subsection{Performance Characteristics}
Compared to previous diffusion models, FLUX 1.1 Pro Ultra demonstrates superior performance in two key areas. First, it is capable of generating images at high resolutions—up to 4 megapixels—without a proportional increase in generation time, enabling detailed and high-fidelity outputs. Second, it exhibits a high degree of prompt fidelity, resulting in more accurate and contextually appropriate visual representations of the input text \cite{blackforestlabs2024flux}. This characteristic is particularly important for ensuring a generated image closely reflects the semantic content of a detailed textual prompt.
