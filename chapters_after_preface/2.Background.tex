\chapter{Background}
\label{chap:background}

The transformation of textual content within images into new visual representations requires a confluence of several advanced technologies. A comprehensive understanding of the field necessitates a technical foundation in the core components that enable such a process. This chapter provides the requisite background, beginning with an examination of Optical Character Recognition (OCR), with a focus on the Tesseract engine. Subsequently, it delves into the architecture and capabilities of modern Large Language Models (LLMs), using GPT-OSS-20B as a representative example. The chapter concludes with a review of state-of-the-art text-to-image (T2I) synthesis, exemplified by the FLUX 1.1 Pro Ultra model.

\section{Optical Character Recognition with Tesseract}
\label{sec:background_ocr}
OCR is a field of computer vision dedicated to the automated extraction of text from images, thereby converting unstructured visual data into a machine-readable format. This technology is fundamental to the digitization of printed and handwritten materials. While early OCR systems relied on template matching, the field has transitioned toward deep learning-based methods that offer superior accuracy and robustness \cite{esser2020improving}.

The Tesseract OCR engine, originally developed by Hewlett-Packard and now maintained by Google, is a prominent open-source implementation that exemplifies this modern approach. Its architecture involves a sequence of processing stages. Initially, the engine performs image preprocessing, which includes adaptive thresholding, noise removal, and skew correction, to enhance text clarity. Following this, a page layout analysis algorithm identifies and isolates text regions from other elements like images or tables. These regions are then subjected to word and character segmentation using techniques such as connected component analysis. The core of the recognition process is a Long Short-Term Memory (LSTM) based neural network, which processes character sequences and leverages contextual dependencies to improve accuracy. Finally, a post-processing stage applies language modeling and dictionary-based corrections to refine the output and mitigate misclassification errors.

A significant advantage of Tesseract is its support for custom model training. While the engine includes pre-trained models for numerous languages, accuracy can be substantially improved for specific domains by fine-tuning the model on a custom dataset. This process involves several systematic steps: collecting a diverse and representative dataset of text images, annotating these images with corresponding ground-truth text, and using Tesseract's training utilities to train the neural network. The resulting specialized model can then be evaluated and optimized to achieve higher performance on challenging, domain-specific images than a general-purpose model can provide.

\section{Large Language Models for Text Processing}
\label{sec:background_llm}
GPT-OSS-20B is an open-weight LLM from the gpt-oss series \cite{openai2025gptoss}. It is a notable example of a powerful model designed with a balance of performance and suitability for local deployment.

\subsection{Architecture of GPT-OSS-20B}
GPT-OSS-20B is a decoder-only Transformer with approximately 21 billion parameters. It features a Mixture-of-Experts (MoE) routing mechanism that activates a subset of its parameters—roughly 3.6 billion—for each token during inference. This MoE design enhances computational throughput and reduces the active parameter count per token, thereby improving the trade-off between latency and output quality. The model's architecture begins by converting input text into token embeddings, with positional information added to preserve sequence order for the attention mechanism. It then processes these embeddings through stacked self-attention layers and position-wise feed-forward networks to capture long-range dependencies and increase representational capacity. The use of residual connections and normalization layers throughout the network stabilizes the optimization process. For inference, the model recommends a specific response format to ensure standardized and reliable structured outputs.

\subsection{Pre-training, Alignment, and Adaptation}
The model is pre-trained on a diverse corpus of text using a causal next-token prediction objective, which endows it with broad linguistic and reasoning capabilities. Following pre-training, an alignment phase uses supervised fine-tuning and reinforcement learning from human feedback to improve its adherence to instructions and overall safety, while preserving its tool-use functions. A key feature for local deployment is its compatibility with post-training quantization. The application of MXFP4 quantization to the MoE weights allows GPT-OSS-20B to operate within approximately 16GB of memory with minimal degradation in output quality. This facilitates its deployment on consumer-grade hardware. Released under the Apache-2.0 license, the model's open-weight nature permits further adaptation via supervised fine-tuning for domain-specific tasks.

\section{Text-to-Image Synthesis with FLUX 1.1 Pro Ultra}
\label{sec:background_t2i}
The field of text-to-image (T2I) synthesis has seen rapid advancement, with models like FLUX 1.1 Pro Ultra representing the state of the art \cite{blackforestlabs2024flux}. Such models are selected for high performance in image quality, adherence to complex prompts, and generation speed.

\subsection{Core Architecture and Methodology}
FLUX 1.1 Pro Ultra is a 12-billion-parameter model built on a hybrid architecture that combines multimodal transformers with diffusion-based generative processes. This design integrates diffusion and flow-matching techniques to improve sample efficiency and the coherence of the generated images. The architecture employs parallelized attention layers, which contribute to greater computational efficiency and faster inference times compared to traditional serial approaches. Furthermore, the inclusion of Rotary Positional Embeddings (RoPE) improves the model's spatial understanding, enabling higher fidelity in complex compositions. To enhance prompt comprehension, the model uses both textual and visual embeddings as multimodal inputs.

The training regimen for FLUX 1.1 Pro Ultra involved several advanced techniques. The training dataset was enriched through extensive data augmentation, including the use of synthetic captioning methods. A rectified flow timestep sampling approach was employed to improve training efficiency, and the model was expanded to its large parameter count in accordance with empirically derived scaling laws.

\subsection{Performance Characteristics}
Compared to previous diffusion models, FLUX 1.1 Pro Ultra demonstrates superior performance in two key areas. First, it is capable of generating images at high resolutions—up to 4 megapixels—without a proportional increase in generation time, enabling detailed and high-fidelity outputs. Second, it exhibits a high degree of prompt fidelity, resulting in more accurate and contextually appropriate visual representations of the input text \cite{blackforestlabs2024flux}. This characteristic is particularly important for ensuring a generated image closely reflects the semantic content of a detailed textual prompt.