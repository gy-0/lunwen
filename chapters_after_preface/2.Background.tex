\chapter{Background}

Recent advancements in Artificial Intelligence have greatly improved text recognition, natural language processing, and image generation. OCR has transitioned from traditional rule-based methods to deep learning-driven approaches, with Tesseract emerging as one of the most widely used open-source OCR engines. Tesseract’s evolution, incorporating machine learning techniques, has significantly enhanced text extraction accuracy across diverse applications. Large language models (LLMs), including the open-weight gpt-oss series (e.g., gpt-oss-20b), have further revolutionized text generation and understanding, while generative models such as FLUX are pushing the boundaries of image synthesis. This chapter explores these technologies, their development, and their real-world applications.


\section{OCR}
\subsection{Overview of OCR}
OCR is a branch within computer vision and artificial intelligence that involves the automatic identification of texts from images and scanned documents. Applications involving OCR have played a very vital role in digitizing printed and handwritten text, thus facilitating applications related to document digitization, automatic data entry, and accessibility software for people who cannot see or see poorly. The evolution of OCR has proceeded from template matching and statistical modeling to the current deep learning-based approaches realizing state-of-the-art accuracy and robustness.

The classic OCR systems were mainly based on pattern recognition approaches where characters were identified with the help of predefined templates concerning their shape and structure. With machine learning and especially deep learning, this field has taken a complete U-turn by selecting features from the data directly instead of designing them manually.

Most of the modern OCR pipelines are multi-staged: pre-processing, segmentation, feature extraction, classification, and post-processing. Pre-processing includes noise removal, binarization, and geometric normalization steps that enhance the quality of the input images. Segmentation refers to the division of an image into separate characters or words to enable its recognition. Feature extraction-conventional, hand-engineered, or deep learning-based-is one of the important steps to distinguish the text from the background artifacts. The final classification is done based on models of CNNs, RNNs, or a combination of both that carry out the classification of text content with high precision.

\subsection{The Tesseract Model}

Tesseract is one of the most widely used open-source OCR engines, originally developed by Hewlett-Packard and later open-sourced by Google. It has undergone significant evolution over the years, incorporating advanced machine learning techniques to improve recognition accuracy and performance. Tesseract supports multiple languages and scripts, making it a versatile tool for diverse OCR applications.

The core architecture of Tesseract involves several processing steps:

\begin{enumerate}
    \item \textbf{Image Pre-processing:} Tesseract performs adaptive thresholding, noise removal, and skew correction to prepare the input image for further processing. This step is crucial for enhancing text visibility and minimizing artifacts that could affect recognition accuracy.
    
    \item \textbf{Page Layout Analysis:} Tesseract employs an intelligent page segmentation algorithm to identify text regions within the document. It can differentiate between text, images, and table structures, enabling more precise recognition.
    
    \item \textbf{Character Segmentation:} After identifying text regions, Tesseract segments individual words and characters using connected component analysis and contour detection techniques. This step ensures proper alignment and separation of characters for accurate recognition.
    
    \item \textbf{Feature Extraction and Recognition:} Tesseract uses a Long Short-Term Memory (LSTM)-based neural network for recognizing text. The LSTM model processes character sequences, capturing contextual dependencies to enhance recognition accuracy.
    
    \item \textbf{Post-processing:} To refine recognition results, Tesseract applies language modeling and dictionary-based correction techniques. These approaches help mitigate errors arising from misclassification and improve output quality.
\end{enumerate}

Tesseract supports multiple output formats, including plain text, searchable PDFs, and structured data formats such as XML and HOCR. It also provides customizable parameters to fine-tune recognition based on specific application requirements, such as character whitelist/blacklist, OCR engine modes, and page segmentation modes.

\subsubsection{Training Custom Tesseract Models}

While Tesseract offers pre-trained models for numerous languages, users can train custom models to enhance recognition accuracy for domain-specific applications. The training process involves the following steps:

\begin{enumerate}
    \item \textbf{Data Collection:} Collecting a diverse and representative dataset of text images, ensuring it covers variations in font styles, sizes, and image conditions.
    
    \item \textbf{Annotation:} Labeling the collected images with corresponding ground-truth text data. Tools such as Tesseract's training tool or third-party annotation software can be used for this purpose.
    
    \item \textbf{Feature Extraction and Training:} Using Tesseract's training utilities, feature extraction is performed, followed by training the neural network to learn character patterns and sequences.
    
    \item \textbf{Evaluation and Optimization:} After training, the model is evaluated using test datasets to assess accuracy and identify areas for improvement. Fine-tuning and augmentation techniques can be employed to optimize performance.
\end{enumerate}

Training a custom Tesseract model allows for improved performance in specialized domains such as medical document processing, historical document digitization, and industrial automation.

\subsubsection{Integration of Tesseract with Other Technologies}

Tesseract can be integrated with other technologies to extend its capabilities. For instance, combining OCR with Natural Language Processing (NLP) allows for intelligent text processing, such as entity recognition and sentiment analysis. Additionally, integrating Tesseract with image processing frameworks like OpenCV enables advanced pre-processing techniques to improve OCR results further.

Furthermore, Tesseract can be deployed in cloud-based solutions, leveraging scalability and computational power to process large volumes of documents efficiently. Popular cloud providers offer OCR services that incorporate Tesseract alongside proprietary algorithms to deliver high-accuracy results.

\subsubsection{Challenges and Future Directions}

Despite its capabilities, Tesseract OCR faces challenges such as handling low-quality images, recognizing cursive handwriting, and supporting complex scripts. Ongoing research in deep learning, transformer-based models, and generative approaches aims to address these challenges and push OCR technology to new frontiers.

Future advancements in OCR, including real-time processing, multilingual recognition, and automated error correction, are expected to further enhance the usability and effectiveness of OCR systems in various domains.





\section{gpt-oss-20b and Large Language Models}

gpt-oss-20b is an open-weight large language model released by OpenAI as part of the gpt-oss series, alongside the larger gpt-oss-120b. With roughly 21 billion parameters in a decoder-only Transformer with mixture-of-experts (MoE) routing (about 3.6B active parameters per token), it targets lower-latency and local or specialized deployments under a permissive Apache-2.0 license \cite{openai2025gptoss}. The model is optimized for reasoning and agentic use cases, supports function calling and structured outputs, and can be served efficiently on commodity GPUs when quantized.

\subsection{Architecture of gpt-oss-20b}

gpt-oss-20b follows a decoder-only Transformer design augmented with mixture-of-experts routing to improve throughput and reduce the active compute per token. Input text is first converted into token embeddings, with positional information injected to preserve order for the attention mechanism. Stacked self-attention layers with multiple heads capture long-range dependencies and contextual interactions across the sequence, while position-wise feed-forward networks increase representational capacity. Residual connections and normalization layers stabilize optimization and enable deeper networks. The MoE router then selects a small subset of experts per token, yielding approximately 3.6B active parameters at inference while maintaining a total parameter budget near 21B, thereby improving latency–quality trade-offs in practical deployments. For inference-time prompting, the series recommends a structured “Harmony” response format that standardizes function calling and structured outputs \cite{openai2025gptoss}.

\subsection{Pre-training, Alignment, and Adaptation}

The model is pre-trained via causal next-token prediction on diverse text corpora to acquire broad linguistic competence and reasoning behavior. Subsequent alignment employs supervised fine-tuning and human feedback to shape helpfulness, safety, and adherence to instructions while preserving tool-use capabilities. In deployment, configurable reasoning effort allows practitioners to select low-, medium-, or high-effort decoding profiles to balance latency against reasoning depth. Furthermore, post-training MXFP4 quantization of MoE weights enables gpt-oss-20b to operate within approximately 16GB of memory without significant loss in quality, facilitating local or on-premise serving. Because the weights are openly released under a permissive license, the model can be further adapted through supervised fine-tuning for domain-specific tasks, and prompts should follow the Harmony chat template to ensure robust function calling and structured outputs \cite{openai2025gptoss}.

\subsection{Capabilities and Applications}

In empirical use, gpt-oss-20b demonstrates strong performance in text generation and editing, code completion and explanation, and the summarization and question answering of long-form content. Through structured outputs and function calling, it integrates naturally with external tools, retrieval systems, and programmatic APIs to support agentic workflows. The MoE execution and quantization make the model attractive for local or specialized deployments where latency and memory are constrained, while fine-tuning provides a pathway to tailor the model to domain-specific requirements \cite{openai2025gptoss}.




\section{Background on FLUX 1.1 Pro Ultra Model}

\subsection{Introduction to FLUX 1.1 Pro Ultra}
FLUX 1.1 Pro Ultra is an advanced text-to-image generation model developed by Black Forest Labs, which represents a significant evolution in the field of generative artificial intelligence. The model leverages a hybrid architecture combining multimodal and parallel diffusion transformer blocks, enabling it to achieve state-of-the-art performance in terms of image quality, prompt adherence, and generation speed. FLUX 1.1 Pro Ultra scales to an impressive 12 billion parameters, significantly surpassing earlier models such as Stable Diffusion XL.

\subsection{Historical Development}
Black Forest Labs, founded in 2024 by former Stability AI researchers, pioneered the development of FLUX 1.1 Pro Ultra with the goal of improving image generation quality and efficiency. The team, composed of researchers from Ludwig Maximilian University of Munich, previously contributed to the development of Stable Diffusion, a widely used text-to-image model. Black Forest Labs introduced FLUX 1.1 Pro Ultra in August 2024.

\subsection{Core Architecture}
FLUX 1.1 Pro Ultra is built upon a unique combination of multimodal transformers and diffusion-based generative processes. The core components of the architecture include:

\begin{itemize}
    \item \textbf{Hybrid Model Design:} FLUX 1.1 Pro Ultra integrates both diffusion and flow-matching techniques to improve sample efficiency and image coherence. Diffusion models operate by progressively refining noise into a coherent image, while flow matching optimizes the latent space trajectory for better guidance.
    \item \textbf{Parallel Attention Layers:} Unlike traditional diffusion models, FLUX 1.1 Pro Ultra employs parallelized attention mechanisms, which contribute to increased computational efficiency and faster inference times.
    \item \textbf{Rotary Positional Embeddings (RoPE):} The incorporation of RoPE improves spatial understanding within images, enabling the model to maintain high fidelity in complex compositions.
    \item \textbf{Multimodal Inputs:} The model takes advantage of textual and visual embeddings to enhance its prompt comprehension and visual output quality.
\end{itemize}

\subsection{Training Methodology}
The training process of FLUX 1.1 Pro Ultra involves several cutting-edge techniques to enhance model performance:

\begin{itemize}
    \item \textbf{Data Augmentation:} Extensive data augmentation strategies, including synthetic captioning inspired by OpenAI's research, have been employed to enrich the model's training dataset.
    \item \textbf{Timestep Sampling:} A novel rectified flow timestep sampling approach improves the efficiency of the training phase by optimizing the learning trajectory.
    \item \textbf{Scaling Laws:} The team followed empirical scaling laws to expand the model size up to 12 billion parameters while maintaining computational feasibility.
\end{itemize}

\subsection{Performance Enhancements}
FLUX 1.1 Pro Ultra introduces several key improvements over previous diffusion models:

\begin{itemize}
    \item \textbf{Higher Resolution Generation:} The Ultra mode enables the generation of images with resolutions up to 4 megapixels without sacrificing speed.
    \item \textbf{Prompt Fidelity:} Compared to competitors like Midjourney V6 and DALL-E 3, FLUX 1.1 Pro Ultra excels in maintaining a high degree of prompt fidelity, ensuring accurate visual representations.
\end{itemize}
